{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8fb7e08-decd-45f2-b8d9-18568d4f7c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "import sys\n",
    "from typing import Tuple, List, Union, Optional\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "import PIL.Image as Image\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "import clip\n",
    "import PIL\n",
    "import random\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "tokenizer = clip.tokenize\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_Tokenizer = _Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe95d3a-5140-4719-99e5-e845d37ae377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from enum import Enum\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "from typing import Tuple, Optional, Union\n",
    "\n",
    "\n",
    "        \n",
    "class MappingType(Enum):\n",
    "    MLP = 'mlp'\n",
    "    Transformer = 'transformer'\n",
    "\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "class ClipGpt2Model(nn.Module):\n",
    "\n",
    "    def __init__(self,prefix_size: int = 512):\n",
    "        super(ClipGpt2Model, self).__init__()\n",
    "        with open('./decoder_config.pkl','rb') as f:\n",
    "            config = pickle.load(f)\n",
    "        self.decoder = GPT2LMHeadModel(config)\n",
    "        self.embedding_size = self.decoder.transformer.wte.weight.shape[1]\n",
    "        self.clip_project = MLP((prefix_size,self.embedding_size))\n",
    "        \n",
    "    def forward(self, clip_features,gpt_tokens):\n",
    "        embedding_text = self.decoder.transformer.wte(gpt_tokens)\n",
    "        embedding_clip = self.clip_project(clip_features)\n",
    "        embedding_clip = embedding_clip.reshape(-1,1,self.embedding_size)\n",
    "        embedding_cat = torch.cat([embedding_clip,embedding_text],dim=1)\n",
    "        out = self.decoder(inputs_embeds=embedding_cat)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c9bf6e-1503-4c23-9b06-b3c36958e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decoding(model,clip_features):\n",
    "    model.eval()\n",
    "    embedding_cat = model.clip_project(clip_features).reshape(1,1,-1)\n",
    "    entry_length = 30\n",
    "    temperature = 1\n",
    "    tokens = None\n",
    "    for i in range(entry_length):\n",
    "        # print(location_token.shape)\n",
    "        outputs = model.decoder(inputs_embeds=embedding_cat)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "        logits_max = logits.max()\n",
    "        logits = torch.nn.functional.softmax(logits)\n",
    "        next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
    "        next_token_embed = model.decoder.transformer.wte(next_token)\n",
    "\n",
    "        if tokens is None:\n",
    "            tokens = next_token\n",
    "\n",
    "        else:\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "        if next_token.item()==49407:\n",
    "            break\n",
    "        embedding_cat = torch.cat((embedding_cat, next_token_embed), dim=1)\n",
    "    try:\n",
    "        output_list = list(tokens.squeeze().cpu().numpy())\n",
    "        output = _Tokenizer.decode(output_list)\n",
    "    except:\n",
    "        output = 'None'\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6c0d992-9f92-4d18-8168-b9241daf8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClipGpt2Model()\n",
    "weights_path = './coco_model/coco_prefix-009.pt'\n",
    "model.load_state_dict(torch.load(weights_path,map_location= torch.device('cpu')))\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2070507-fd6f-46c8-9d71-94565b0e70b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.27it/s]\n"
     ]
    }
   ],
   "source": [
    "## construct the support memory\n",
    "with open('./coco_train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "data = random.sample(data,10000)\n",
    "text_features = []\n",
    "captions = []\n",
    "batch_size = 1000\n",
    "clip_model.eval()\n",
    "for i in tqdm(range(0,len(data[:])//batch_size)):\n",
    "    \n",
    "    texts = data[i*batch_size:(i+1)*batch_size]\n",
    "    with torch.no_grad():\n",
    "        texts_token = tokenizer(texts).to(device)\n",
    "        text_feature = clip_model.encode_text(texts_token)\n",
    "        text_features.append(text_feature)\n",
    "        captions.extend(texts)\n",
    "\n",
    "text_features = torch.cat(text_features,dim=0)\n",
    "text_features /= text_features.norm(dim=-1,keepdim=True).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160b069-65db-4aa3-80b6-14a926861b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pic = '1.png'\n",
    "image = preprocess(Image.open(path_pic)).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    image_features /= image_features.norm(dim=-1,keepdim=True)\n",
    "    sim = image_features@text_features.T.float()\n",
    "    sim = nn.functional.softmax(sim*100)\n",
    "    prefix_embedding = sim@text_features.float()\n",
    "    prefix_embedding /= prefix_embedding.norm(dim=-1,keepdim=True)\n",
    "    generated_text = Decoding(model,prefix_embedding)\n",
    "    generated_text = generated_text_prefix.replace('<|startoftext|>','').replace('<|endoftext|>','')\n",
    "    print(generated_text_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nuplan]",
   "language": "python",
   "name": "conda-env-nuplan-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
